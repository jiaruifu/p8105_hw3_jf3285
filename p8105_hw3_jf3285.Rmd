---
title: "p8105_hw2_jf3285"
author: "Jiarui Fu"
date: "10/14/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
```

## Problem 1
This problem uses the Instacart data. The goal is to do some exploration of this dataset.  
Q1. There are 134 aisles. Aisle 83 (fresh vegetables, first most) and aisle 24 (fresh fruits, second most) are the most items ordered from.  
  Q2. The plot is shown below. Aisles are arranged by their respective IDs. It can be seen that the two aisles with the most number of items ordered are fresh vegetables and fresh fruits, which corresponds to the finding in Q1.
```{r}
# load the data
library(p8105.datasets)
data("instacart")

instacart %>% 
  # Q1: How many aisles are there, and which aisles are the most items ordered from?
  # count the number of times each aisle is ordered
  group_by(aisle_id, aisle) %>% 
  summarise(item_ordered_from_aisle=n()) %>% 
  
  # Q2: make a plot that shows the number of items ordered in each aisle
  # limit to aisles with more than 10000 items ordered
  filter(item_ordered_from_aisle > 10000) %>% 
  ggplot(aes(x = aisle_id, y = item_ordered_from_aisle)) + 
  geom_point(aes(color = aisle)) + 
  labs(title = "The number of items ordered in each aisle",
       x = "Aisle ID",
       y = "Number of Items Ordered") + theme(text = element_text(size=8))
```

Q3. The table is shown below. The three most popular items in aisle baking ingredients are: Light Brown Sugar, Pure Baking Soda, Cane Sugar (list by the number of times each item is ordered). The three most popular items in aisle dog food care are: Snack Sticks Chicken & Rice Recipe Dog Treats, Organix Chicken & Brown Rice Recipe, and Small Dog Biscuits (list by the number of times each item is ordered). The three most popular items in aisle packaged vegetable fruits are: Organic Baby Spinach, Organic Raspberries and Organic Blueberries (list by the number of times each item is ordered). Overall, the number of times items in aisle packaged vegetable fruits are ordered are much higher than the number of times items in aisle baking ingredients and dog food care are ordered.
```{r}
# load the data
library(p8105.datasets)
data("instacart") 

# Q3: make a table showing the three most popular items in the specified aisles below
instacart %>% 
  # limit to only three aisles
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  # count the number of times each item is ordered
  group_by(aisle, product_name) %>%
  summarize(number_of_times_item_ordered = n()) %>% 
  # count(product_name, name = "number_of_times_item_ordered") %>% 
  # keep the three most popular items in each of the aisles
  filter(min_rank(desc(number_of_times_item_ordered)) < 4) %>% 
  knitr::kable()
```

Q4. The table is shown below. The mean hour of the day at which Coffee Ice Cream are ordered on each day of the week is 13.8, 14.3, 15.4, 15.3, 15.2, 12.3, 13.8 from Sunday to Saturday. The mean hour of the day at which Pink Lady Apples are ordered on each day of the week is 13.4, 11.4, 11.7, 14.2, 11.6, 12.8, 11.9 from Sunday to Saturday. Generally, the mean hour of the day at which Coffee Ice Cream is ordered on each day of the week is later than that of Pink Lady Apples. 
```{r}
# load the data
library(p8105.datasets)
data("instacart")

# Q4: make a table showing the mean hour of the day at which the specified products below are ordered on each day of the week
# change order_dow's data type to factor
instacart$order_dow = as.factor(instacart$order_dow)
instacart %>% 
  # recode order_dow into each day of the week based on 0-6 => Sun to Sat
  mutate(order_dow = recode(order_dow, 
                            "0" = "Sun", "1" = "Mon", "2" = "Tue", "3" = "Wed", 
                            "4" = "Thu", "5" = "Fri", "6" = "Sat")) %>% 
  # limit to only two products
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  # count the mean hour of the day at which they are ordered on each day of the week 
  group_by(order_dow, product_name) %>% 
  summarise(mean_hour = mean(order_hour_of_day)) %>% 
  # produce a 2*7 table 
  pivot_wider(id_cols = c(order_dow, product_name, mean_hour),
              names_from = order_dow,
              values_from = mean_hour) %>% 
  knitr::kable(digits = 1)
```

Dataset description: 
The instacart dataset has 1384617 rows and 15 columns. The dataset is relatively tidy, given that each variable forms a column, each observation forms a row and each type of observational unit forms a table. Some key variables are add_to_cart_order[int] - the order of the item being added to cart in each order, reordered[int]- the number of times the item being ordered again, order_dow[int] - the day of the week the item being ordered, order_hour_of_day[int] - the hour of the day the item being ordered, days_since_prior_order[int] - the number of days the item being reordered again since its last order, aisle_id[int] - the aisle number, department_id[int] - the department number, aisle[chr] - aisle name, department[chr] - department name, product_name[chr] - the name of the product, etc. 

For example, the order_dow ranges from (0, 6), representing Sunday to Saturday in a week. The order_hour_of_day ranges from (`r range(pull(instacart, order_hour_of_day))`), representing the 24 hours in a day; the mean order_hour_of_day is `r mean(pull(instacart, order_hour_of_day))` with a standard deviation of `r sd(pull(instacart, order_hour_of_day))` and the median order_hour_of_day is `r median(pull(instacart, order_hour_of_day))`. The days_since_prior_order ranges from (`r range(pull(instacart, days_since_prior_order))`); the mean days_since_prior_order is `r mean(pull(instacart, days_since_prior_order))` with a standard deviation of `r sd(pull(instacart, days_since_prior_order))` and the median days_since_prior_order is `r median(pull(instacart, days_since_prior_order))`.

## Problem 2
This problem uses the BRFSS data.  
  Q1. In 2002, 6 states were observed at 7 or more locations, including CT, FL, MA, NC, NJ, PA. In 2010, 14 states were observed at 7 or more locations, including CA, CO, FL, MA, MD, NC, NE, NJ, NY OH, PA, SC, TX, WA.
```{r, message=FALSE}
# load the data
library(p8105.datasets)
data("brfss_smart2010")

# data cleaning
# change response's data type to factor
brfss_smart2010$Response = as.factor(brfss_smart2010$Response)
tidy_data = 
brfss_smart2010 %>% 
  janitor::clean_names() %>%
  # format the data to use appropriate variable names
  rename(state = locationabbr,
         county = locationdesc) %>% 
  # focus on the "Overall Health" topic
  filter(topic == "Overall Health") %>% 
  # include only responses from "Excellent" to "Poor"
  filter (response %in% c("Excellent", "Very good", "Good", "Fair", "Poor")) %>% 
  # organize responses as a factor taking levels ordered from “Poor” to “Excellent”
  mutate(response = forcats::fct_relevel(response, c("Poor", "Fair", "Good", "Very good", "Excellent"))) %>% 
  arrange(response)
tidy_data

# Q1: In 2002, which states were observed at 7 or more locations? What about in 2010?
# year 2002
tidy_data %>% 
  # set year = 2002
  filter(year == 2002) %>% 
  # count the number of locations that each state was observed at
  group_by(state) %>% 
  summarise(number_of_locations = n_distinct(county)) %>% 
  # set observations at 7 or more locations
  filter(number_of_locations >= 7)
# year 2010
tidy_data %>% 
  # set tear = 2010
  filter(year == 2010) %>% 
  # count the number of locations that each state was observed at
  group_by(state) %>% 
  summarise(number_of_locations = n_distinct(county)) %>%
  # set observations at 7 or more locations
  filter(number_of_locations >= 7)

# Q2
# construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state
average_state_locations = 
tidy_data %>% 
  # limit to Excellent responses
  filter(response == "Excellent") %>%
  # contain year, state
  group_by(year, state) %>% 
  # create a new variable that averages the data_value across locations within a state
  summarise(avg_data_value = mean(data_value)) 
average_state_locations

# make a “spaghetti” plot of this average value over time within a state
# make a plot showing a line for each state across years
average_state_locations %>% 
  ggplot(aes(x = year, y = avg_data_value, color = state)) + 
  geom_point() +
  geom_line(aes(group = state)) + 
  labs(title = "Average value of locations within a state over time",
       x = "Year",
       y = "Average value of locations") + theme(text = element_text(size=10))

# Q3
# make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State
tidy_data %>% 
  # set year = 2006 and 2010
  filter(year %in% c(2006, 2010)) %>% 
  # set location in NY State
  filter(state == "NY") %>% 
  # two-panel plot
  ggplot(aes(x = response, y = data_value)) +
  geom_boxplot() + 
  labs(title = "Distribution of data_value for responses among locations in NY state",
       x = "response",
       y = "data_value") + theme(text = element_text(size=10)) +
  facet_grid(~year)
```

## Problem 3
This problem uses five weeks of accelerometer data collected on a 63 year-old male with BMI 25. 
```{r, message=FALSE}
accel_data = 
  # load, tidy the data
  read_csv("data/accel_data.csv") %>%
  janitor::clean_names() %>%
  # include a weekday vs weekend variable
  mutate(weekday_vs_weekend =
         ifelse(day %in% c("Saturday", "Sunday"), "weekend", "weekday")) %>% 
  # include all originally observed variables and values 
  select(week, day_id, day, weekday_vs_weekend, everything())

# aggregate across minutes to create a total activity variable for each day
# create a table showing these totals
accel_data %>%  
  mutate(total_activity_daily = rowSums(.[5:1444])) %>% 
  select(week, day_id, day, weekday_vs_weekend, total_activity_daily) %>% 
  knitr::kable()

# tidied dataset
# change the wider format to longer
# create a new variable "minute" to replace the activity for each minute, and a new variable "activity" to represent the corresponding activity count
clean_data = 
  pivot_longer(accel_data, cols = activity_1:activity_1440,
               names_to = "minute",
               values_to = "activity"
               ) %>% 
  # "activity_" is removed from the entire minute column to reflect time (minutes)
  mutate_all(~gsub("activity_", "", .)) %>% 
  # change the data type for week and day_id to numeric
  mutate(week = as.numeric(week),
         day_id = as.numeric(day_id)) %>%
   # change the data type for day and weekday_vs_weekend to factor
  mutate(day = as.factor(day),
         weekday_vs_weekend = as.factor(weekday_vs_weekend)) %>%
  # change the data type for minute and activity to numeric
  mutate(minute = as.numeric(minute),
         activity = as.numeric(activity)) %>% 
  select(week, day_id, day, weekday_vs_weekend, everything()) %>% 
  # group by each day
  group_by(day_id)
clean_data

# create a new dataset to reformat the tidied dataset
data_by_hour = 
  clean_data %>%
  # minute is transformed to hour using quotient
  # 1440 minutes -> 24 hours
  mutate(hour = minute%/%60) %>% 
  # create a unique day variable
  mutate(unique_day = paste0("Week ",week,": ",day)) %>%
  group_by(week, day, unique_day, hour) %>% 
  # sum the hourly activity 
  mutate(hourly_activity = sum(activity)) %>% 
  select(day, unique_day, hour, hourly_activity) %>% 
  # remove duplicate rows
  distinct() 

# create five different datasets for Week 1 to 5
wk1 = 
data_by_hour %>% 
  filter (week == 1)

wk2 = 
  data_by_hour %>% 
  filter (week == 2)

wk3 = data_by_hour %>% 
  filter (week == 3)

wk4 = data_by_hour %>% 
  filter (week == 4) 

wk5 = data_by_hour %>% 
  filter (week == 5)

# make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week
# 35 lines - 35 days in total
# 7 legends - group by day of the week
ggplot() +
geom_line(wk1, mapping = aes(x = hour, y = hourly_activity, color = day)) +
geom_line(wk2, mapping = aes(x = hour, y = hourly_activity, color = day)) +
geom_line(wk3, mapping = aes(x = hour, y = hourly_activity, color = day)) +
geom_line(wk4, mapping = aes(x = hour, y = hourly_activity, color = day)) +
geom_line(wk5, mapping = aes(x = hour, y = hourly_activity, color = day)) +
labs(title = "24-Hour Activity Time Courses For Each Day",
       x = "Hour",
       y = "Activity") + theme(text = element_text(size=12)) 
```

Q1. The final dataset includes the week number from 1 to 5 representing the 5 weeks in the study [dbl], the day_id from 1 to 35 representing the  35 days in the study [dbl], the day representing the day of the week from Monday to Sunday [fctr], the weekday vs weekend variable representing if the day is weekday or weekend [fctr], minute representing the minute of a day from 1 to 1440 for each day [dbl], and minute activity [dbl]. The mean minute activity is `r mean(pull(clean_data, activity))` with a standard deviation of `r sd(pull(clean_data, activity))` and the median minute activity is `r median(pull(clean_data, activity))`.  
  Q2. In the table that shows the total activity by each day of the week, there are not many apparent trends. But it can be easily seen that the total activities for Week 1 Monday, Week 4 Saturday and Week 5 Saturday are extremely low compared to those of other days.  
    Q3. In the plot that shows the 24-hour activity time courses for each day, it can be seen that there are about four peaks during the five-week study where the peaks of activity for each day are around Thursday at 6am, Sunday at 11am, Saturday at 16pm, and Monday and Friday at 20pm. 
