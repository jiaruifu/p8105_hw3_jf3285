---
title: "p8105_hw2_jf3285"
author: "Jiarui Fu"
date: "10/14/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
```

## Problem 1
This problem uses the Instacart data.
```{r Instacart}
# load the data
library(p8105.datasets)
data("instacart") 

instacart %>% 
  # Q1: How many aisles are there, and which aisles are the most items ordered from?
  # count the number of times each aisle is ordered
  group_by(aisle_id, aisle) %>% 
  summarise(item_ordered_from_aisle=n()) %>% 
  
  # make a plot that shows the number of items ordered in each aisle
  # limit to aisles with more than 10000 items ordered
  filter(item_ordered_from_aisle > 10000) %>% 
  ggplot(aes(x = aisle_id, y = item_ordered_from_aisle)) + 
  geom_point(aes(color = aisle)) + 
  labs(title = "The number of items ordered in each aisle",
       x = "Aisle ID",
       y = "Number of Items Ordered") + theme(text = element_text(size=8))

# make a table showing the three most popular items in the specified aisles below
instacart %>% 
  # limit to only three aisles
  filter(aisle == c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  # count the number of times each item is ordered
  group_by(aisle, product_name) %>%
  summarise(number_of_times_item_ordered=n()) %>% 
  # keep the three most popular items in each of the aisles
  filter(min_rank(desc(number_of_times_item_ordered)) < 4) %>% 
  knitr::kable()

# make a table showing the mean hour of the day at which the specified products are ordered on each day of the week
# change order_dow's data type to factor
instacart$order_dow = as.factor(instacart$order_dow)
instacart %>% 
  # recode order_dow into each day of the week based on 0-6 = Sun to Sat
  mutate(order_dow = recode(order_dow, 
                            "0" = "Sun", "1" = "Mon", "2" = "Tue", "3" = "Wed", 
                            "4" = "Thu", "5" = "Fri", "6" = "Sat")) %>% 
  # limit to only two products
  filter(product_name == c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  # count the mean hour of the day at which they are ordered on each day of the week 
  group_by(order_dow, product_name) %>% 
  summarise(mean_hour = mean(order_hour_of_day)) %>% 
  # produce a 2*7 table 
  pivot_wider(id_cols = c(order_dow, product_name, mean_hour),
              names_from = order_dow,
              values_from = mean_hour) %>% 
  knitr::kable(digits = 1)
```

The instacart dataset has 1384617 rows and 15 columns. Some key variables are...

Q1. There are 134 aisles. Aisle 83 (fresh vegetables, most) and aisle 24 (fresh fruits, second most) are the most items ordered from.

## Problem 2

```{r BRFSS}
# load the data
library(p8105.datasets)
data("brfss_smart2010")

# change response's data type to factor
brfss_smart2010$Response = as.factor(brfss_smart2010$Response)
# data cleaning
tidy_data = 
brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  # focus on the "Overall Health" topic
  filter(topic == "Overall Health") %>% 
  # include only responses from "Excellent" to "Poor"
  filter (response == c("Excellent", "Very good", "Good", "Fair", "Poor")) %>% 
  # organize responses as a factor taking levels ordered from “Poor” to “Excellent”
  mutate(response = forcats::fct_relevel(response, c("Poor", "Fair", "Good", "Very good", "Excellent"))) %>% 
  arrange(response)
tidy_data

# Q1: In 2002, which states were observed at 7 or more locations? What about in 2010?
# year 2002
tidy_data %>% 
  # set year = 2002
  filter(year == 2002) %>% 
  # count the number of locations that each state was observed at
  group_by(locationabbr) %>% 
  summarise(number_of_locations = n()) %>% 
  # set observations at 7 or more locations
  filter(number_of_locations >= 7)
# year 2010
tidy_data %>% 
  # set tear = 2010
  filter(year == 2010) %>% 
  # count the number of locations that each state was observed at
  group_by(locationabbr) %>% 
  summarise(number_of_locations = n()) %>%
  # set observations at 7 or more locations
  filter(number_of_locations >= 7)

# Q2
tidy_data %>% 
  # limit to Excellent responses
  filter(response == "Excellent") %>%
  # contain year, state
  group_by(year, locationabbr) %>% 
  # create a new variable that averages the data_value across locations within a state
  summarise(avg_data_value = mean(data_value)) %>% 
  # make a “spaghetti” plot of this average value over time within a state
  # make a plot showing a line for each state across years
  ggplot(aes(x = year, y = avg_data_value, color = locationabbr)) + 
  geom_point() +
  geom_line(aes(group = locationabbr)) + 
  labs(title = "Average value of locations within a state over time",
       x = "Year",
       y = "Average value of locations") + theme(text = element_text(size=10))

# Q3
# make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State
tidy_data %>% 
  # set year = 2006 and 2010
  filter(year == c(2006, 2010)) %>% 
  # set location in NY State
  filter(locationabbr == "NY") %>% 
  # two-panel plot
  ggplot(aes(x = response, y = data_value, color = locationdesc)) +
  geom_point() + 
  labs(title = "Distribution of data_value for responses among locations in NY state",
       x = "response",
       y = "data_value") + theme(text = element_text(size=8)) +
  facet_grid(~year)
```

Q1. In 2002, 21 states were observed at 7 or more locations, including CT, DE, FL, ID, IL, IN, MA, MD, MI, NE, NH, NJ, NV, NY, OH, OK, PA, RI, SC, UT and WA.   
    In 2010, 38 states were observed at 7 or more locations, including AL, AR, AZ, CA, CO, DE, FL, ID, IN, KS, LA, MA, MD, ME, MI, MN, MO, MT, NC, NE, NH, NJ, NM, NV, NY, OH, OK,OR, PA, RI, SC, SD, TN, TX, UT, VT, WA AND WY. 

## Problem 3
```{r accelerometer, message = FALSE}
accel_data = 
  # load, tidy the data
  read_csv("data/accel_data.csv") %>%
  janitor::clean_names() %>%
  # include a weekday vs weekend variable
  mutate(weekday_vs_weekend =
         ifelse(day %in% c("Saturday", "Sunday"), "weekend", "weekday")) %>% 
  select(week, day_id, day, weekday_vs_weekend, everything())
accel_data

accel_data %>%  
  mutate(total_activity_daily = rowSums(.[5:1444])) %>% 
  select(week, day_id, day, weekday_vs_weekend, total_activity_daily)
  



  
  

```
